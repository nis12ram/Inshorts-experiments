{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EBYvs6tSpZM",
        "outputId": "2b97f373-97a6-4a02-abd3-0f222b1efbf1"
      },
      "outputs": [],
      "source": [
        "!pip install datasets==3.6.0 evaluate rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E6KRvItiv-xf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import  ModuleList\n",
        "import json\n",
        "from typing import List, Dict, Callable, Any\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers.models.qwen2.modeling_qwen2 import Qwen2DecoderLayer, Qwen2Model, Qwen2Attention, Qwen2ForCausalLM, Qwen2RMSNorm, Qwen2RotaryEmbedding, Qwen2MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JIS9YJDE1-QM"
      },
      "outputs": [],
      "source": [
        "def get_num_params(model,only_require_grad = False, in_gb = False, bytes_per_param = 2):\n",
        "    num_params = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if only_require_grad:\n",
        "          if param.requires_grad:\n",
        "              num_params += param.numel()\n",
        "        else:\n",
        "          num_params += param.numel()\n",
        "\n",
        "    return f\"{round((num_params*bytes_per_param)/10**9,2)}gb\" if in_gb else f\"{num_params}params\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c0lZn0Y94U-O"
      },
      "outputs": [],
      "source": [
        "def model_initialization(model_name_or_path):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name_or_path,\n",
        "      torch_dtype=torch.float16,\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "  return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kDmxkuTVtu60"
      },
      "outputs": [],
      "source": [
        "def replace_modulelist_in_model(model, modulelist: torch.nn.ModuleList):\n",
        "  setattr(model.model, \"layers\", modulelist)\n",
        "  ## updating model's config\n",
        "  model.config.num_hidden_layers = len(modulelist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J-kFc2WA4ZdI"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "def remove_model_from_gpu_memory(m):\n",
        "  del m\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JIt-V6_p74We"
      },
      "outputs": [],
      "source": [
        "def find_parent(model, name: str) -> torch.nn.Module:\n",
        "    module_tree = name.split(\".\")[:-1]\n",
        "    parent = model\n",
        "    for m in module_tree:\n",
        "        parent = parent._modules[m]\n",
        "    return parent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QQ3C-lcm9gdg"
      },
      "outputs": [],
      "source": [
        "def move_to_nearest_even(hidden_dim):\n",
        "  if hidden_dim % 2 == 0:\n",
        "    return hidden_dim\n",
        "  else:\n",
        "    return hidden_dim + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjHrGJRjdg3l"
      },
      "source": [
        "## dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nngxKpEv3_99",
        "outputId": "d2d7805d-26ca-48dc-86ea-5bbe3d72ab09"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"shivam9980/Inshorts-english\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ajZcnQL4TVv",
        "outputId": "4ad710aa-9a11-4884-fed9-eeaa42b3640f"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iumLfJq74UqP"
      },
      "outputs": [],
      "source": [
        "ds = ds.train_test_split(test_size = 0.025, seed = 3407)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErfyPxeV5MF6",
        "outputId": "a73de84c-3cad-4587-c01b-69f18354610c"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lF0DyZhMQZYU"
      },
      "outputs": [],
      "source": [
        "user_prompt = '''Generate a concise news headline based on the following news content. The headline should clearly and accurately summarize the key point of the article. Avoid exaggeration or misleading phrasing.\n",
        "\n",
        "News Content: {content}'''\n",
        "\n",
        "input_prompt = '''<|im_start|>system\n",
        "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
        "<|im_start|>user\n",
        "Generate a concise news headline based on the following news content. The headline should clearly and accurately summarize the key point of the article. Avoid exaggeration or misleading phrasing.\n",
        "\n",
        "News Content: {content}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "W0JpgE_t5jfE"
      },
      "outputs": [],
      "source": [
        "def map_func(datapoint):\n",
        "  datapoint[\"text\"] = f'''<|im_start|>system\n",
        "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
        "<|im_start|>user\n",
        "Generate a concise news headline based on the following news content. The headline should clearly and accurately summarize the key point of the article. Avoid exaggeration or misleading phrasing.\n",
        "\n",
        "News Content: {datapoint[\"Content\"]}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "{datapoint[\"Headline\"]}<|im_end|>'''\n",
        "\n",
        "  datapoint[\"input\"] = f'''<|im_start|>system\n",
        "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
        "<|im_start|>user\n",
        "Generate a concise news headline based on the following news content. The headline should clearly and accurately summarize the key point of the article. Avoid exaggeration or misleading phrasing.\n",
        "\n",
        "News Content: {datapoint[\"Content\"]}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "'''\n",
        "  return datapoint\n",
        "ds = ds.map(map_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UE6TVeHcUOWw"
      },
      "outputs": [],
      "source": [
        "calibration_dataset = ds[\"train\"].select(range(612))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpUWIr-GVV1k",
        "outputId": "f8e84dc0-e9a6-4cea-eb83-93ee3d8181b3"
      },
      "outputs": [],
      "source": [
        "calibration_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nQI8O7PJuZLS"
      },
      "outputs": [],
      "source": [
        "downstream_accuracy_evaluator_dataset = ds[\"test\"].select(range(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jiTUFryvaD6",
        "outputId": "dcfd065d-e54c-4012-d51e-844281a13b28"
      },
      "outputs": [],
      "source": [
        "downstream_accuracy_evaluator_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFk45vgQvqhH"
      },
      "source": [
        "## downstream accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2apcG-VEvvNw"
      },
      "outputs": [],
      "source": [
        "def get_output(model, tokenizer, inputs, sampling_params):\n",
        "  model_inputs = tokenizer(inputs, padding=True, padding_side='left', return_tensors=\"pt\").to(model.device)\n",
        "  generated_ids = model.generate(\n",
        "      **model_inputs,\n",
        "      max_new_tokens=100,\n",
        "      **sampling_params\n",
        "\n",
        "  )\n",
        "  generated_ids = [\n",
        "      output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "  ]\n",
        "  response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8jjk_AZBgvBN"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "rouge = evaluate.load('rouge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CqaXD00_g60C"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_rouge(predictions, references):\n",
        "  results = rouge.compute(predictions=predictions,\n",
        "                        references=references)\n",
        "  return results\n",
        "\n",
        "def handle_rouge_results(results):\n",
        "  ## initialization\n",
        "  final_score = {}\n",
        "  for key in results[0].keys():\n",
        "    final_score[key] = []\n",
        "\n",
        "  ## collection\n",
        "  for result in results:\n",
        "    for key in result.keys():\n",
        "      final_score[key].append(result[key])\n",
        "\n",
        "  ## averaging\n",
        "  for key in final_score.keys():\n",
        "    final_score[key] = sum(final_score[key])/len(final_score[key])\n",
        "  return final_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CgB-xLMdhB0Z"
      },
      "outputs": [],
      "source": [
        "def run_downstream_accuracy_test(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    dataset: Dataset,\n",
        "    batch_size: int,\n",
        "    reference_field: str,\n",
        "    to_print: bool,\n",
        "    sampling_params: Dict[str, str],\n",
        "    batch_callback: Callable | None = None,\n",
        "    batch_callback_args: Dict[Any, Any] | None = None\n",
        "  ):\n",
        "  rouge_results: List[Dict[str, float]] = []\n",
        "  ds_iter = dataset.iter(batch_size)\n",
        "  for batch in tqdm(ds_iter):\n",
        "    predictions: List[str] = get_output(model, tokenizer, batch[\"input\"], sampling_params)\n",
        "    references: List[str] = batch[reference_field]\n",
        "    rouge_result: Dict[str, float] = compute_rouge(predictions, references)\n",
        "    rouge_results.append(rouge_result)\n",
        "\n",
        "    if to_print:\n",
        "      print(\"*\"*6)\n",
        "      for pred, refe in zip(predictions, references):\n",
        "        print(\"-\"*3)\n",
        "        print(f\"pred: {pred}\")\n",
        "        print(f\"refe: {refe}\")\n",
        "        print(\"-\"*3)\n",
        "      print(\"*\"*6)\n",
        "\n",
        "    if batch_callback:\n",
        "      if batch_callback_args:\n",
        "        batch_callback(**batch_callback_args)\n",
        "      else:\n",
        "        batch_callback()\n",
        "\n",
        "  dataset_rouge_result: Dict[str, float] = handle_rouge_results(rouge_results)\n",
        "  return dataset_rouge_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfP-8OeDvfMu"
      },
      "source": [
        "## depth pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bajKDE1uwaLW"
      },
      "outputs": [],
      "source": [
        "def trim_layers(model, layers_index_to_trim: List[int]):\n",
        "  encoder_layers = model.model.layers\n",
        "  for layer_index_to_trim in sorted(layers_index_to_trim, reverse=True):\n",
        "    del encoder_layers[layer_index_to_trim]\n",
        "    ## updating model's config\n",
        "    model.config.num_hidden_layers -= 1\n",
        "\n",
        "  ## updating layer_idx value\n",
        "  for new_layer_idx, layer in enumerate(encoder_layers):\n",
        "    for module in layer.modules():\n",
        "      if hasattr(module, \"layer_idx\"):\n",
        "        module.layer_idx = new_layer_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rvdNtvodbdGJ"
      },
      "outputs": [],
      "source": [
        "def run_depth_pruning_experiments_from_model(model, tokenizer, collection_of_layers_index_to_trim: List[List[int]], batch_size: int, run_original_first: bool=True, also_init_best_model: bool = False):\n",
        "  original_decoder_layers_copy = deepcopy(model.model.layers)\n",
        "\n",
        "  if run_original_first:\n",
        "    print(\"#\"*3, \"ORIGINAL\",\"#\"*3)\n",
        "    model_size = get_num_params(model,in_gb=True)\n",
        "    print(f\"ORIGINAL MODEL SIZE: {model_size}\")\n",
        "    result = run_downstream_accuracy_test(model=model, tokenizer=tokenizer, dataset=downstream_accuracy_evaluator_dataset, batch_size=batch_size, reference_field=\"Headline\", to_print=True, sampling_params=dict(do_sample=False, temperature=None, top_p=None, top_k=None))\n",
        "    print(result)\n",
        "    print(\"#\"*3, \"ORIGINAL\",\"#\"*3)\n",
        "\n",
        "  capture_results = []\n",
        "  for layers_index_to_trim in collection_of_layers_index_to_trim:\n",
        "    print(\"#\"*10)\n",
        "    print(f\"layers_index_to_trim: {layers_index_to_trim}\")\n",
        "    replace_modulelist_in_model(model=model, modulelist=deepcopy(original_decoder_layers_copy)) ## original model\n",
        "    # print(len(original_decoder_layers_copy))\n",
        "    trim_layers(model, layers_index_to_trim) ## prunned model\n",
        "    model_size = get_num_params(model,in_gb=True)\n",
        "    print(f\"NEW MODEL SIZE: {model_size}\")\n",
        "    result = run_downstream_accuracy_test(model=model, tokenizer=tokenizer, dataset=downstream_accuracy_evaluator_dataset, batch_size=batch_size, reference_field=\"Headline\", to_print=True, sampling_params=dict(do_sample=False, temperature=None, top_p=None, top_k=None))\n",
        "    print(result)\n",
        "    capture_results.append({\"result\":result, \"layers_index_to_trim\":layers_index_to_trim})\n",
        "    print(\"#\"*10,end =\"\\n\\n\")\n",
        "\n",
        "  best_result = None\n",
        "  best_rougeL = 0.0\n",
        "  for result in capture_results:\n",
        "    rougeL = result[\"result\"][\"rougeL\"]\n",
        "    if rougeL > best_rougeL:\n",
        "      best_rougeL = rougeL\n",
        "      best_result = result\n",
        "\n",
        "  replace_modulelist_in_model(model=model, modulelist=original_decoder_layers_copy) ## original model\n",
        "  if also_init_best_model:\n",
        "    trim_layers(model, best_result[\"layers_index_to_trim\"]) ## best pruned model\n",
        "\n",
        "  return dict(best_result=best_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMu_H_d04dKA"
      },
      "source": [
        "## width pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qWza91wG5pKF"
      },
      "outputs": [],
      "source": [
        "def mlp_neuron_importance_hook(module, input, output):\n",
        "  '''\n",
        "  B -> batch size\n",
        "  S -> Sequence length\n",
        "  in -> Input size\n",
        "  out -> Output size\n",
        "  ## input[0] (B, S, in)\n",
        "  ## output (B, S, out)\n",
        "  '''\n",
        "  output_copy = output.clone().detach() ## (B, S, out)\n",
        "  output_copy = torch.abs(output_copy)\n",
        "  batch_size, _, _ = output_copy.size()\n",
        "  if batch_size != 1:\n",
        "    raise NotImplementedError(\"ONLY BATCH SIZE=1 IS IMPLEMENTED\")\n",
        "  single_batch_output = output_copy[0] ## (S, out)\n",
        "  module.neurons_activation = torch.cat((module.neurons_activation, single_batch_output), 0) if hasattr(module,\"neurons_activation\") else single_batch_output ## (_, out)\n",
        "  del output_copy, single_batch_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "N9YG8f1E6vfa"
      },
      "outputs": [],
      "source": [
        "def register_all_forward_hooks(model):\n",
        "  for (name, module) in model.named_modules():\n",
        "    if isinstance(module, Qwen2DecoderLayer):\n",
        "      ## mlp neuron_pruning\n",
        "      module.mlp.gate_proj.register_forward_hook(mlp_neuron_importance_hook)\n",
        "\n",
        "def remove_all_forward_hooks(model):\n",
        "  for (name, module) in model.named_modules():\n",
        "    if isinstance(module, Qwen2DecoderLayer):\n",
        "      ## mlp neuron_pruning\n",
        "      module.mlp.gate_proj._forward_hooks.clear()\n",
        "\n",
        "def remove_all_forward_hooks_stored_info(model):\n",
        "  for (name, module) in model.named_modules():\n",
        "    if isinstance(module, Qwen2DecoderLayer):\n",
        "      ## mlp neuron_pruning\n",
        "      if hasattr(module.mlp.gate_proj,\"neurons_activation\"):\n",
        "        delattr(module.mlp.gate_proj,\"neurons_activation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gMQne52O8uMk"
      },
      "outputs": [],
      "source": [
        "def prune_mlp_neuron(model, importance_fn, fraction_to_prune = None, pruned_dim = None):\n",
        "  for (name, module) in model.named_modules():\n",
        "    if isinstance(module, Qwen2DecoderLayer):\n",
        "      ## access importance\n",
        "      importance = importance_fn(module) ## (out,)\n",
        "\n",
        "      ## setting number of neurons to keep\n",
        "      if not pruned_dim and not fraction_to_prune:\n",
        "        raise ValueError(\"Both fraction_to_prune and pruned_dim are None. Please specify one.\")\n",
        "      num_of_most_imp_neurons_to_keep = pruned_dim if pruned_dim else move_to_nearest_even(\n",
        "          int((1 - fraction_to_prune) * importance.shape[0])\n",
        "      )\n",
        "\n",
        "      ## setting which neurons to keep based on their magnitude(activation) value\n",
        "      indices = importance.argsort(descending = True)[:num_of_most_imp_neurons_to_keep] ## (out*,)\n",
        "\n",
        "      ## accessing the original modules\n",
        "      gate_proj = module.mlp.gate_proj\n",
        "      up_proj = module.mlp.up_proj\n",
        "      down_proj = module.mlp.down_proj\n",
        "\n",
        "      ## pruning weights and biases\n",
        "      gate_proj_weight = gate_proj.weight.data.clone() ## (out, in)\n",
        "      pruned_gate_proj_weight = gate_proj_weight[indices, :] ## (out*, in)\n",
        "      pruned_gate_proj_bias = None\n",
        "      if gate_proj.bias is not None:\n",
        "        gate_proj_bias = gate_proj.bias.data.clone() ## (out)\n",
        "        pruned_gate_proj_bias = gate_proj_bias[indices] ## (out*)\n",
        "\n",
        "      up_proj_weight = up_proj.weight.data.clone() ## (out, in)\n",
        "      pruned_up_proj_weight = up_proj_weight[indices, :] ## (out*, in)\n",
        "      pruned_up_proj_bias = None\n",
        "      if up_proj.bias is not None:\n",
        "        up_proj_bias = up_proj.bias.data.clone() ## (out)\n",
        "        pruned_up_proj_bias = up_proj_bias[indices] ## (out*)\n",
        "\n",
        "      down_proj_weight = down_proj.weight.data.clone() ## (out, in)\n",
        "      pruned_down_proj_weight = down_proj_weight[:, indices] ## (out, in*)\n",
        "      pruned_down_proj_bias = None\n",
        "      if down_proj.bias is not None:\n",
        "        down_proj_bias = down_proj.bias.data.clone() ## (out)\n",
        "        pruned_down_proj_bias = down_proj_bias ## (out)\n",
        "\n",
        "      ## constructing pruned modules\n",
        "      pruned_gate_proj = nn.Linear(gate_proj.in_features, num_of_most_imp_neurons_to_keep, bias=gate_proj.bias is not None).to(\n",
        "        model.device\n",
        "      )\n",
        "      pruned_up_proj = nn.Linear(up_proj.in_features, num_of_most_imp_neurons_to_keep, bias=up_proj.bias is not None).to(\n",
        "        model.device\n",
        "      )\n",
        "      pruned_down_proj = nn.Linear(num_of_most_imp_neurons_to_keep, down_proj.out_features, bias=down_proj.bias is not None).to(\n",
        "        model.device\n",
        "      )\n",
        "\n",
        "      ## storing pruned weights and biases to pruned modules\n",
        "      pruned_gate_proj.weight.data = pruned_gate_proj_weight ## (out*, in)\n",
        "      if pruned_gate_proj_bias is not None:\n",
        "        pruned_gate_proj.bias.data = pruned_gate_proj_bias ## (out*)\n",
        "\n",
        "      pruned_up_proj.weight.data = pruned_up_proj_weight ## (out*, in)\n",
        "      if pruned_up_proj_bias is not None:\n",
        "        pruned_up_proj.bias.data = pruned_up_proj_bias ## (out*)\n",
        "\n",
        "      pruned_down_proj.weight.data = pruned_down_proj_weight ## (out, in*)\n",
        "      if pruned_down_proj_bias is not None:\n",
        "        pruned_down_proj.bias.data = pruned_down_proj_bias ## (out)\n",
        "\n",
        "      ## replacing original modules with pruned_modules\n",
        "      setattr(module.mlp, \"gate_proj\", pruned_gate_proj)\n",
        "      setattr(module.mlp, \"up_proj\", pruned_up_proj)\n",
        "      setattr(module.mlp, \"down_proj\", pruned_down_proj)\n",
        "\n",
        "      ## updating intermediate_size value in Qwen2MLP modules\n",
        "      setattr(module.mlp, \"intermediate_size\",  num_of_most_imp_neurons_to_keep)\n",
        "\n",
        "  ## updating intermediate_size value in model's config\n",
        "  setattr(model.model.config, \"intermediate_size\", num_of_most_imp_neurons_to_keep)\n",
        "\n",
        "  ## updating model's config in modules\n",
        "  for (name, module) in model.named_modules():\n",
        "    if hasattr(module, \"config\"):\n",
        "      setattr(module, \"config\", model.model.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sbQ62MTLL3qa"
      },
      "outputs": [],
      "source": [
        "def batch_callback(model):\n",
        "  for (name, module) in model.named_modules():\n",
        "    if isinstance(module, Qwen2DecoderLayer):\n",
        "      ## accessing the concatenated activation values for all token in a single sequence\n",
        "      gate_proj_neurons_activation = module.mlp.gate_proj.neurons_activation ## (_, out)\n",
        "\n",
        "      ## calculating importance by aggregating the concatenated activation values of all tokens in a single sequence\n",
        "      importance = torch.linalg.norm(gate_proj_neurons_activation, ord=2, dim=0, dtype=torch.float32) ## (out,)\n",
        "      # importance = torch.mean(gate_proj_neurons_activation, axis = 0, dtype = torch.float32)\n",
        "      # importance = torch.sum(gate_proj_neurons_activation, axis = 0, dtype = torch.float32)\n",
        "      # importance = torch.std(gate_proj_neurons_activation, dim = 0)\n",
        "      # importance = torch.median(gate_proj_neurons_activation, dim = 0).values\n",
        "      # importance = torch.quantile(gate_proj_neurons_activation, q=0.5, dim = 0)\n",
        "\n",
        "\n",
        "\n",
        "      ## concatenating importance values of all sequence\n",
        "      importance = importance.reshape(1, -1) ## (1, out)\n",
        "      module.mlp.gate_proj.calibration_ds_importance = torch.cat((module.mlp.gate_proj.calibration_ds_importance, importance), dim=0) if hasattr(module.mlp.gate_proj, \"calibration_ds_importance\") else importance ## (_, out)\n",
        "\n",
        "  ## removing the attached hook, deleting the info stored by hook(neurons_activation in these case) and then attaching a new hook\n",
        "  remove_all_forward_hooks(model)\n",
        "  remove_all_forward_hooks_stored_info(model)\n",
        "  register_all_forward_hooks(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hAmXp-IyRja9"
      },
      "outputs": [],
      "source": [
        "def access_importance(module):\n",
        "  importance = torch.linalg.norm(module.mlp.gate_proj.calibration_ds_importance, ord=2, dim=0, dtype=torch.float32) ## (out,)\n",
        "  # importance = torch.mean(module.mlp.gate_proj.calibration_ds_importance, axis=0, dtype=torch.float32) ## (out,)\n",
        "  # importance = torch.sum(module.mlp.gate_proj.calibration_ds_importance, axis=0, dtype=torch.float32) ## (out,)\n",
        "  # importance = torch.median(module.mlp.gate_proj.calibration_ds_importance, dim=0).values ## (out,)\n",
        "\n",
        "  return importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "VkN26cWSPLKf"
      },
      "outputs": [],
      "source": [
        "def run_width_pruning_experiment(model, tokenizer, pruned_dim: int|None, batch_size: int, run_original_first: bool=True):\n",
        "\n",
        "  remove_all_forward_hooks(model)\n",
        "  remove_all_forward_hooks_stored_info(model)\n",
        "\n",
        "  if run_original_first:\n",
        "    print(\"#\"*3, \"ORIGINAL\",\"#\"*3)\n",
        "    model_size = get_num_params(model,in_gb=True)\n",
        "    print(f\"ORIGINAL MODEL SIZE: {model_size}\")\n",
        "    result = run_downstream_accuracy_test(model=model, tokenizer=tokenizer, dataset=downstream_accuracy_evaluator_dataset, batch_size=batch_size, reference_field=\"Headline\", to_print=True, sampling_params=dict(do_sample=False, temperature=None, top_p=None, top_k=None))\n",
        "    print(result)\n",
        "    print(\"#\"*3, \"ORIGINAL\",\"#\"*3)\n",
        "\n",
        "\n",
        "  register_all_forward_hooks(model)\n",
        "  run_downstream_accuracy_test(model=model, tokenizer=tokenizer, dataset=calibration_dataset, batch_size=1, reference_field=\"Headline\", to_print=False, sampling_params=dict(do_sample=False, temperature=None, top_p=None, top_k=None), batch_callback=batch_callback, batch_callback_args=dict(model=model))\n",
        "\n",
        "  remove_all_forward_hooks(model)\n",
        "  remove_all_forward_hooks_stored_info(model)\n",
        "\n",
        "  prune_mlp_neuron(model, fraction_to_prune=None, importance_fn=access_importance, pruned_dim=pruned_dim)\n",
        "  model_size = get_num_params(model, in_gb=True)\n",
        "  print(f\"NEW MODEL SIZE: {model_size}\")\n",
        "\n",
        "  pruned_model_result = run_downstream_accuracy_test(model=model, tokenizer=tokenizer, dataset=downstream_accuracy_evaluator_dataset, batch_size=batch_size, reference_field=\"Headline\", to_print=True, sampling_params=dict(do_sample=False, temperature=None, top_p=None, top_k=None))\n",
        "\n",
        "  return dict(pruned_model_result=pruned_model_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZarw8XcD5RE"
      },
      "source": [
        "## hybrid pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8hC9GKWZBYL"
      },
      "source": [
        "#### width to depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "j08qFuTyZDM_"
      },
      "outputs": [],
      "source": [
        "tokenizer, model = model_initialization(\"nis12ram/qwen2.5-0.5B-Instruct-Inshort\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FaYtBBkZLlb",
        "outputId": "33485fce-2ca2-4d76-90b8-1f0823af828f"
      },
      "outputs": [],
      "source": [
        "width_experiment_result = run_width_pruning_experiment(model=model, tokenizer=tokenizer, pruned_dim=4096, batch_size=5 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6fYH_KiCt44",
        "outputId": "cf33d56a-83dc-4a83-c3b0-3f9bdd9f11fd"
      },
      "outputs": [],
      "source": [
        "width_experiment_result.get('pruned_model_result')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZvNAdFq1Kuz",
        "outputId": "5649d76c-7c98-461c-a877-e5a123cee75a"
      },
      "outputs": [],
      "source": [
        "collection_of_layers_index_to_trim = [\n",
        "  [i, i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8, i+9, i+10,  i+11, i+12]  for i in range(12)\n",
        "]\n",
        "(collection_of_layers_index_to_trim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9GcY5myZPLO",
        "outputId": "9877e444-3988-4ee6-c458-1866c392c036"
      },
      "outputs": [],
      "source": [
        "depth_experiment_result = run_depth_pruning_experiments_from_model(model, tokenizer, collection_of_layers_index_to_trim = collection_of_layers_index_to_trim, batch_size = 5, also_init_best_model= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzstoNbcETVL",
        "outputId": "3d6a8f6a-1545-465b-e35c-b68948319e81"
      },
      "outputs": [],
      "source": [
        "depth_experiment_result.get('best_result')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7pGAeNHNaTc",
        "outputId": "5e290456-6b91-4c6d-afdb-2fe7d3f7ab23"
      },
      "outputs": [],
      "source": [
        "model, model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT5PE0_ROCgj",
        "outputId": "627c4ac8-470c-4aaf-a2f9-a3e7b2dcbf82"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"/content/inshort/models/hybrid7_base\")\n",
        "tokenizer.save_pretrained(\"/content/inshort/models/hybrid7_base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCciiKgjEL5X"
      },
      "source": [
        "## sample inference test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "SaxuFizdEOqs"
      },
      "outputs": [],
      "source": [
        "tokenizer, model = model_initialization(\"/content/inshort/models/hybrid7_base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXg6d0gcEsDV",
        "outputId": "fcde859d-2338-4662-e999-710526eedf53"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpVm7uIbEvM9",
        "outputId": "33cf2e0c-ccd7-4cd8-8cd1-80fb10bf5fa7"
      },
      "outputs": [],
      "source": [
        "output = get_output(model, tokenizer, calibration_dataset[100][\"input\"], dict(do_sample=False, temperature=None, top_p=None, top_k=None))\n",
        "calibration_dataset[100][\"input\"], output"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "CjHrGJRjdg3l",
        "KFk45vgQvqhH",
        "NfP-8OeDvfMu"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
